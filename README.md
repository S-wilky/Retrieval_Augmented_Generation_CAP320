# Retrieval_Augmented_Generation_CAP320

## Step 1: Select and Load a Document
The file I selected to use is "The_Social_and_Cultural_Order_of_Ancient.txt".
I created a separate file called "split_clean_embed.py" to generate the "Selected_Document.txt" file.

## Step 2: Install Required Libraries
ChatGPT gave suggestions for doing it separately or together. I picked together using the folloing line:

```pip install sentence-transformers transformers pandas scikit-learn```

Then I asked how to generate a requirements.txt file automatically and ChatGPT directed me to install pipreqs and use the following command:

```pipreqs .```

## Step 3: Generate Embeddings and Store Locally
Splitting the txt file into chunks and generating embeddings were both added to the "split_clean_embed.py" file.
This program saves the embeddings in the "text_embeddings.json" file.

At this point I ran into some trouble with the "cleaning" part of the code, because it comflicted with the portion of the code that split it. Therefore it all ended up in 1 giant chunk. I ended up removing it entirely.

Then based on the formatting of the document I chose, I had to change to split on a single "\n" instead of double. This made the chunks turn out how I wanted them to.

After implementing the query system, I ran into additional problems because the initial embeddings were coded differently than the embeddings for the query. I ended up just asking chatGPT to do all parts (clean, split, embed, query, embed query, and respond) in the same question so that the codebase was consistent.

## Step 4: Query the System
Sample Queries and responses:
1. Why are Pharoahs important?
   - **Response:** They are special cases thought to be able to live simultaneously in both realm of existence, by their direct genealogical connection to the first ancestors and the creator of the world
3. How did social development start in ancient egypt?
   - **Response:** Ancient Egypt became known as a breadbasket (later used by the Romans), and this development seems closely linked to the gold deposits in the Eastern Desert.
4. How did ancient Egyptians manage work and labor?
   - **Response:** kinship systems

I had to look around in the text a little bit to ensure I was asking questions that were actually talked about. Otherwise I would get extremely basic answers like "Ancient Egypt" or similar. However, once I narrowed the questions down based on the text, the responses turned out much better, though I did have to be careful about how I worded the questions or it would get confused.


## Step 5: Deepen Your Understanding of the Program
My response limit with the newest model ran out at this point, so I had to open a new chat. Therefore I edited my response to the following:
```
What are five important questions I should ask to understand how a program program that
cleans a txt file, embeds it, prompts the user for a query, embeds the query, then generates a response from the text to answer the query works?
Please include questions about concepts such as cosine similarity, sentence-transformers, and the role of embeddings.
```

This was the response generated by ChatGPT:
```
Here are five important questions to ask to understand how a program that cleans a text file, embeds it, prompts the user for a query, embeds the query, and generates a response works:

How does the program clean the text file before embedding?
Cleaning is a crucial step to remove unnecessary elements like stopwords, punctuation, and formatting issues that might affect the quality of the embeddings.
Understanding the cleaning process will clarify the quality of the data being fed into the embedding model.

What embedding model is used, and how does it convert text into embeddings?
Embedding models, such as sentence-transformers, represent textual data as high-dimensional vectors.
Understanding which specific model is being used and its underlying architecture (e.g., BERT, RoBERTa, etc.) will help you understand the program's approach to capturing semantic meaning.

What is the role of cosine similarity in comparing the user's query with the embedded text, and how does it work?
Cosine similarity is a common metric to measure the similarity between two vectors (in this case, the query embedding and document embeddings).
It's essential to understand how cosine similarity works to evaluate how closely related the query is to the available text data.

How does the program generate a response based on the query and the embedded text?
After embedding both the text and the query, the program must generate a response. Does it simply return the most similar sentence or passage?
Or is there more advanced logic involved, such as generating a concise answer by selecting relevant parts of the text?

How are the embeddings stored and retrieved, and how do they scale with larger datasets?
Understanding the storage and retrieval of embeddings is important for performance and scalability.
Does the program use an efficient data structure, like a nearest-neighbor search algorithm (e.g., FAISS), to retrieve the most relevant embeddings when handling large datasets?
These questions cover the major concepts involved in text cleaning, embedding, similarity measurement, and response generation.
```

1. How does the program clean the test file before embedding?*
```
In short, cleaning is about transforming the raw text into a format that's suitable for embedding. Each step ensures that the embeddings better represent the semantic content of the text, improving the performance of downstream tasks such as querying or similarity measurement.
```

2. What embedding model is used, and how does it convert text into embeddings?*
```
In summary, the embedding model (like sentence-transformers) converts text into high-dimensional vectors by considering the context and relationships between words. These embeddings are rich in semantic meaning and can be used for various downstream tasks like answering questions, search, or similarity comparison.
```

3. What is the role of cosine similarity in comparing the user's query with the embedded text, and how does it work?*
```
In summary, cosine similarity is a key tool in embedding-based search systems, allowing the program to effectively compare the user's query with embedded text and determine how relevant the text is to the query based on their semantic similarity.
```

4. How does the program generate a response based on the query and the embedded text?*
```
In Summary:
The program generates a response based on the query and the embedded text by:

  - Converting both the query and the documents into embeddings.
  - Calculating similarity scores (usually cosine similarity) to rank documents by relevance.
  - Extracting or synthesizing the most relevant information based on the top-ranking documents. This approach allows the program to generate responses that are semantically relevant to the user’s query, even when the exact wording doesn’t match between the query and the document text.
```

5. How are the embeddings stored and retrieved, and how do they scale with larger datasets?*
```
In Summary:
  - Storage: Embeddings are stored in optimized formats (in-memory, binary files, or specialized databases) to ensure both speed and scalability.
  - Retrieval: Efficient retrieval is achieved using nearest neighbor search techniques, with ANN methods like FAISS or Annoy providing a significant performance boost.
  - Scaling: For larger datasets, indexing methods, dimensionality reduction, distributed systems, and hybrid storage approaches are employed to scale effectively and handle the computational challenges.
By combining these strategies, systems can handle very large datasets of embeddings efficiently, making it feasible to search and retrieve relevant information quickly even as the corpus grows.
```

*Responses include only the summary to save this file from being hundreds of lines long, though the full responses were a very interesting complement to the 3blue1brown video we watched in week 1.

## Final Notes:
The entire code for this project is stored in the "final_submission.py" file. I had initially tried splitting it up into multiple files as the instructions seemed to suggest, but it did not work out for me at all. That is what all the extra files in the repository are about. Investigate at your own peril. In order to get the program to work, I made some edits as described above. The program works as follows:

I'm not sure if something is wrong with my computer or if you've just got a super teacher computer, but everything in VS code seemed to run at a snails pace compared to yours in lecture. This was the worst part of the project, especially when ChatGPT led me astray and I had to refactor many times.
